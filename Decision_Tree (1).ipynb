{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is a Decision Tree, and how does it work in the context of\n",
        "classification?"
      ],
      "metadata": {
        "id": "2ddaw8s6Oe-0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-> Decision Tree is a supervised machine learning algorithm. it is used for both regression and classification problem. it's used to spilt te data based on feature values. the main aim of the decision tree is to get the pure leaf node for all samples belongs to the same class so prediction makes accurate. in which the leaf node is class label (for classification) and numerical values (for regression).\n",
        "\n",
        "\n",
        "working :\n",
        "\n",
        "Step 1 : starts with entire dataset as root node\n",
        "\n",
        "Step 2 : find the best varibale to split the data into child nodes using:\n",
        "\n",
        "    gini impurity\n",
        "\n",
        "    entropy/ information gain\n",
        "\n",
        "    classification error\n",
        "\n",
        "Step 3 : Split the dataset into subset using the selected feature\n",
        "\n",
        "Step 4 : Repeat the process recursively for each subset until:\n",
        "\n",
        "All data points in a node belong to the same class, or\n",
        "\n",
        "A stopping condition is met (e.g., maximum depth, minimum samples per leaf).\n",
        "\n",
        "Assign class labels at the leaf nodes based on the majority class in that subset."
      ],
      "metadata": {
        "id": "Iv7LCDULOhkl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Explain the concepts of Gini Impurity and Entropy as impurity measures.\n",
        "How do they impact the splits in a Decision Tree?"
      ],
      "metadata": {
        "id": "T6YUA8rAOhuy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-> In decision tree, when it want to split the data in branch and it aim is to make each node pure meaning each sample has one class not any mixed classes in nodes that's why we need to check the impurity in the each node.\n",
        "That's why we use gini impurity, Entropy\n",
        "\n",
        "gini impurity formula is:\n",
        "Gini = 1 - ᶜΣᵢ(pᵢ)^2\n",
        "\n",
        "c = number of classes\n",
        "pᵢ = proportion for class i in node\n",
        "\n",
        "* gini = 0 (pure node)\n",
        "* gini = 0.5 = max impurity (perfectly mixed)\n",
        "\n",
        "lower gini = purer node\n",
        "Gini is directly used for getting best spiliting feature.\n",
        "\n",
        "Entropy :    \n",
        "Entropy is used for impurity measure and also used to measure information gain for getting the best feature to split the node.\n",
        "\n",
        "Entropy = - ᶜΣᵢ pᵢ log pᵢ\n",
        "Entropy = 0 pure node\n",
        "Entropy = 1 maximum impurity"
      ],
      "metadata": {
        "id": "kzhTxJrMOhxU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What is the difference between Pre-Pruning and Post-Pruning in Decision\n",
        "Trees? Give one practical advantage of using each."
      ],
      "metadata": {
        "id": "mhj2FCAcOh07"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pre-Pruning :    \n",
        "\n",
        "Definition:\n",
        "\n",
        "Stop the tree from growing too deep during training.\n",
        "\n",
        "Constraints like: max_depth, min_samples_split, min_samples_leaf.\n",
        "\n",
        "Practical Advantage:\n",
        "\n",
        "Prevents overfitting immediately on small or noisy datasets.\n",
        "\n",
        "Tree is smaller.\n",
        "\n",
        "faster to train and easier to deploy.\n",
        "\n",
        "but major drawback is it might be restricts the training process early without learn all the patterns to the model. it creates the underfitting.\n",
        "\n",
        "Example:\n",
        "\n",
        "clf = DecisionTreeClassifier(max_depth=3, min_samples_split=5)\n",
        "\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "Post-Pruning :\n",
        "\n",
        "Definition:\n",
        "\n",
        "First grow a full tree, then remove branches that don’t improve performance.\n",
        "\n",
        "Typically uses cost-complexity pruning or validation set.\n",
        "\n",
        "Practical Advantage:\n",
        "\n",
        "Allows tree to fully explore patterns first, then remove only unnecessary branches.\n",
        "By this we can achieve higher accuracy than aggressive pre-pruning.\n",
        "\n",
        "Example:\n",
        "\n",
        "path = clf_full.cost_complexity_pruning_path(X_train, y_train)\n",
        "\n",
        "ccp_alphas = path.ccp_alphas\n",
        "\n",
        "clf_post = DecisionTreeClassifier(ccp_alpha=0.01)\n",
        "\n",
        "clf_post.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "xSFgmd-fdKIG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What is Information Gain in Decision Trees, and why is it important for\n",
        "choosing the best split?"
      ],
      "metadata": {
        "id": "Uf-WYuBIdKKz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-> information gain a term for checking which feature is most likely to split the node in branches.\n",
        "High information gain is best feature to split.\n",
        "\n",
        "Information is caluclated by with the help of entropy.\n",
        "\n",
        "formula :\n",
        "\n",
        "IG = Entropy(root) - Entropy(child_combine)\n",
        "\n",
        "Entropy(child_combine) = (no of samples of class_0/total samples in class) * entropy(left_child) + (no of samples of class_1/total samples in class) * entropy(right_child)\n",
        "\n",
        "Importance of choosing the best split :\n",
        "at each node we take decision to get the accurate correct answer. decision tree is nothing but the set of decisions we take at each step (node) before spliting based on conditions.\n",
        "That's why it is important if we take a wrong feature so it will give us wrong answer and wrong prediction."
      ],
      "metadata": {
        "id": "bPX8T5c6dKOD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What are some common real-world applications of Decision Trees, and\n",
        "what are their main advantages and limitations?"
      ],
      "metadata": {
        "id": "TzWT1en5dKWH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-> Applications :\n",
        "\n",
        "1. Banking and Finance : Credit Risk score, Loan approval, fraud detection\n",
        "2. Healthcare : disease diagnosis, Patient risk prediction\n",
        "3. Stock Market : Stock price and up and down predictions\n",
        "4. Marketing : Customer Segmentation, Churn Prediction\n",
        "5. Entertainment : Movie's Genre Classification\n",
        "\n",
        "-> Advantages :\n",
        "\n",
        "1. Used for both classification and Regression\n",
        "2. Easy to interpret\n",
        "3. No need for feature scaling\n",
        "4. Handles non linear relationship well\n",
        "5. Feature Highlights\n",
        "\n",
        "-> Limitations :\n",
        "\n",
        "1. Overfitting : maximum depth and large dataset makes the model suitable for training data and on unseen data it not works.\n",
        "2. If we change in data can produce the different trees."
      ],
      "metadata": {
        "id": "RvV3gl2Phjh_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Train a Decision Tree Classifier using the Gini criterion\n",
        "● Print the model’s accuracy and feature importances"
      ],
      "metadata": {
        "id": "ioVC3RG0hjkn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "feature_names = iris.feature_names\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.3f}\")\n",
        "\n",
        "importances = clf.feature_importances_\n",
        "for name, importance in zip(feature_names, importances):\n",
        "    print(f\"{name}: {importance:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PjIwzThzl_Dq",
        "outputId": "be43dc2a-d64e-49b1-a786-2b98505a0a56"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.000\n",
            "sepal length (cm): 0.000\n",
            "sepal width (cm): 0.017\n",
            "petal length (cm): 0.906\n",
            "petal width (cm): 0.077\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to\n",
        "a fully-grown tree."
      ],
      "metadata": {
        "id": "kikH5_knhjne"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "clf_limited = DecisionTreeClassifier(criterion='gini', max_depth=3, random_state=42)\n",
        "clf_limited.fit(X_train, y_train)\n",
        "y_pred_limited = clf_limited.predict(X_test)\n",
        "accuracy_limited = accuracy_score(y_test, y_pred_limited)\n",
        "print(f\"Accuracy (max_depth=3): {accuracy_limited:.3f}\")\n",
        "\n",
        "clf_full = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf_full.fit(X_train, y_train)\n",
        "y_pred_full = clf_full.predict(X_test)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "print(f\"Accuracy (fully-grown): {accuracy_full:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4tsYuruPnCBp",
        "outputId": "50beb67e-2010-4ac5-8f92-1c95e9546fb8"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (max_depth=3): 1.000\n",
            "Accuracy (fully-grown): 1.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Tune the Decision Tree’s max_depth and min_samples_split using\n",
        "GridSearchCV\n",
        "● Print the best parameters and the resulting model accuracy"
      ],
      "metadata": {
        "id": "25vMxovzhjrE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# iris = load_iris()\n",
        "# X = iris.data\n",
        "# Y = iris.target\n",
        "\n",
        "# X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state = 32, test_size = 0.3)\n",
        "\n",
        "model = DecisionTreeClassifier(criterion = 'gini', random_state = 32)\n",
        "\n",
        "param_grid = {\n",
        "    'max_depth' : [2,3,4,8,6],\n",
        "    'min_samples_split' : [5,2,4,3]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    model,\n",
        "    param_grid = param_grid,\n",
        "    cv = 5,\n",
        "    scoring = 'accuracy'\n",
        ")\n",
        "\n",
        "grid_search.fit(X_train, y_train)\n",
        "print(grid_search.best_params_)\n",
        "\n",
        "y_pred = grid_search.best_estimator_.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy with best parameters: {accuracy:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llPHBKKpoAii",
        "outputId": "7416196c-130d-4cd3-9f3b-6c8a61ed46a1"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'max_depth': 4, 'min_samples_split': 2}\n",
            "Accuracy with best parameters: 1.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. : Imagine you’re working as a data scientist for a healthcare company that\n",
        "wants to predict whether a patient has a certain disease. You have a large dataset with mixed data types and some missing values.\n",
        "\n",
        "Explain the step-by-step process you would follow to:\n",
        "\n",
        "● Handle the missing values\n",
        "\n",
        "● Encode the categorical features\n",
        "\n",
        "● Train a Decision Tree model\n",
        "\n",
        "● Tune its hyperparameters\n",
        "\n",
        "● Evaluate its performance\n",
        "\n",
        "And describe what business value this model could provide in the real-world setting."
      ],
      "metadata": {
        "id": "qaBYjtmrr5K0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Handle Missing Values\n",
        "\n",
        "Numerical → fill with median\n",
        "\n",
        "Categorical → fill with mode\n",
        "\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "for skewed dataset\n",
        "\n",
        "num_imputer = df['FeatureA'].fillna(df['FeatureA'].median())\n",
        "cat_imputer = df['FeatureA'].fillna(df['FeatureA'].mode()[0])\n",
        "\n",
        "2. Encode Categorical Features\n",
        "\n",
        "Low cardinality → One-Hot Encoding\n",
        "\n",
        "High cardinality → Ordinal / Target Encoding\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "encoder = OneHotEncoder(drop='first', sparse=False)\n",
        "\n",
        "3. Train Decision Tree\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "4. Hyperparameter Tuning\n",
        "\n",
        "Tune max_depth, min_samples_split, criterion with GridSearchCV\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "grid_search = GridSearchCV(clf, param_grid, cv=5)\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "5. Evaluate Performance\n",
        "\n",
        "Metrics: Accuracy, Precision, Recall, F1\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(y_test, best_model.predict(X_test))\n",
        "\n",
        "6. Business Value\n",
        "\n",
        "Early disease detection → faster treatment\n",
        "\n",
        "Optimize hospital resources → cost savings\n",
        "\n",
        "Explainable predictions → doctors trust the model"
      ],
      "metadata": {
        "id": "3P3vvKUyr7XQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZE7b6-3eoXFK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}